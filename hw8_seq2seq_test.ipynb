{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence 介紹\n",
    "- 大多數常見的 **sequence-to-sequence (seq2seq) model** 為 **encoder-decoder model**，主要由兩個部分組成，分別是 **Encoder** 和 **Decoder**，而這兩個部分則大多使用 **recurrent neural network (RNN)** 來實作，主要是用來解決輸入和輸出的長度不一樣的情況\n",
    " - **Encoder** 是將**一連串**的輸入，如文字、影片、聲音訊號等，編碼為**單個向量**，這單個向量可以想像為是整個輸入的抽象表示，包含了整個輸入的資訊\n",
    " - **Decoder** 是將 Encoder 輸出的單個向量逐步解碼，**一次輸出一個結果**，直到將最後目標輸出被產生出來為止，每次輸出會影響下一次的輸出，一般會在開頭加入 \"< BOS >\" 來表示開始解碼，會在結尾輸出 \"< EOS >\" 來表示輸出結束\n",
    "![seq2seq](hw8_data/Encoder+DEcoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業介紹\n",
    "- 英文翻譯中文\n",
    "  - 輸入： 一句英文 （e.g. tom is a student .） \n",
    "  - 輸出： 中文翻譯 （e.g. 湯姆 是 個 學生 。）\n",
    "\n",
    "- TODO\n",
    "  - Teachering Forcing 的功用: 嘗試不用 Teachering Forcing 做訓練\n",
    "  - 實作 Attention Mechanism\n",
    "  - 實作 Beam Search\n",
    "  - 實作 Schedule Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入需要的 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 结构\n",
    "- Data (出自manythings 的 cmn-eng): 訓練資料：18000句；檢驗資料：  500句；測試資料： 2636句\n",
    "- Dataset 格式:\n",
    "  - 不同語言的句子用 TAB ('\\t') 分開\n",
    "  - 字跟字之間用空白分開\n",
    "   > it 's none of your concern . \t這不關 你 的 事 。 <br>\n",
    "   > she has a habit of bi@@ ting her na@@ ils . \t她 有 咬 指甲 的 習慣 。 <br>\n",
    "   > he is a teacher . \t他 是 老師 。<br>\n",
    "   > japan re@@ lies on ar@@ a@@ b countries for oil . \t日本 靠 阿拉伯 國家 提供 石油 。 <br>\n",
    "   > i 'll dream about you . \t我會 夢到 你 的 。 \n",
    "\n",
    "- 字典：\n",
    "  - int2word_*.json: 將整數轉為文字\n",
    "   > {\"0\": \"<PAD>\", \"1\": \"<BOS>\", \"2\": \"<EOS>\", \"3\": \"<UNK>\", \"4\": \"。\", \"5\": \"我\", \"6\": \"的\", \"7\": \"了\", \"8\": \"你\", \"9\": \"他\", \"10\": \"？\", \"11\": \"在\", \"12\": \"是\", \"13\": \"她\", \"14\": \"湯姆\", \"15\": \"嗎\", \"16\": \"我們\", \"17\": \"，\", \"18\": \"不\", \"19\": \"有\", \"20\": \"很\", \"21\": \"什麼\", \"22\": \"去\", \"23\": \"做\", \"24\": \"說\", \"25\": \"這個\", \"26\": \"一個\", \"27\": \"知道\", \"28\": \"都\", \"29\": \"他們\", \"30\": \"喜歡\", \"31\": \"把\", \"32\": \"想\", \"33\": \"好\", \"34\": \"這\", \"35\": \"來\", \"36\": \"?\", \"37\": \"沒\", \n",
    "\n",
    "  - word2int_*.json: 將文字轉為整數\n",
    "   > {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3, \".\": 4, \"i\": 5, \"the\": 6, \"to\": 7, \"you\": 8, \"a\": 9, \"?\": 10, \"is\": 11, \"he\": 12, \"n't\": 13, \"tom\": 14, \"do\": 15, \"in\": 16, \"it\": 17, \"'s\": 18, \"of\": 19, \"my\": 20, \"she\": 21, \"have\": 22, \"me\": 23, \"this\": 24, \"that\": 25, \",\": 26, \"was\": 27, \"for\": 28, \"we\": 29, \"are\": 30, \"what\": 31, \"your\": 32, \"on\": 33, \"his\": 34, \"at\": 35, \"like\": 36, \"did\": 37, \"be\": 38, \"not\": 39, \"'m\": 40, \"with\": 41, \"can\": 42, \"her\": 43, \"go\": 44, \"there\": 45, \"has\": 46, \"will\": 47, \"know\": 48, \"him\": 49, \"and\": 50, \"want\": 51, \"how\": 52, \"very\": 53, \"they\": 54,\n",
    "  - $*$ 分為英文（en）和中文（cn）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 轉換\n",
    "將不同長度的答案拓展到相同長度，以便訓練模型\n",
    "\n",
    "```python\n",
    "numpy.pad(array, pad_width, mode = 'constant', **kwargs)\n",
    "```\n",
    "- **array** 表示需要填充的数组；\n",
    "- **pad_width** 表示每个轴（axis）边缘需要填充的数值数目。 参数输入方式为：（(before_1, after_1), …(before_N, after_N)），其中(before_1,after_1)表示第1轴两边缘分别填充before_1个和after_1个数值。取值为：{sequence, array_like,int}\n",
    "- **mode** 表示填充的方式（取值：str字符串或用户提供的函数）,总共有11种填充模式\n",
    "  - ‘constant’——表示连续填充相同的值，每个轴可以分别指定填充值，constant_values=（x,y）时前面用x填充，后面用y填充，缺省值填充0\n",
    "  - ‘edge’——表示用边缘值填充\n",
    "  - ‘linear_ramp’——表示用边缘递减的方式填充\n",
    "  - ‘maximum’——表示最大值填充\n",
    "  - ‘mean’——表示均值填充\n",
    "  - ‘median’——表示中位数填充\n",
    "  - ‘minimum’——表示最小值填充\n",
    "  - ‘reflect’——表示对称填充\n",
    "  - ‘symmetric’——表示对称填充\n",
    "  - ‘wrap’——表示用原数组后面的值填充前面，前面的值填充后面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransform(object): # 填充\n",
    "    def __init__(self, size, pad):\n",
    "        self.size = size\n",
    "        self.pad = pad\n",
    "\n",
    "    def __call__(self, label):\n",
    "        label = np.pad(label, (0, (self.size - label.shape[0])), mode = 'constant', constant_values = self.pad)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預處理 Dataset\n",
    "将词转换成整数\n",
    "![](hw8_data/word_to_int.png)\n",
    "- 資料預處理:\n",
    "  - 英文：\n",
    "    - 用 subword-nmt 套件將word轉為subword\n",
    "    - 建立字典：取出標籤中出現頻率高於定值的subword\n",
    "  - 中文：\n",
    "    - 用 jieba 將中文句子斷詞\n",
    "    - 建立字典：取出標籤中出現頻率高於定值的詞\n",
    "  - 特殊字元： < PAD >, < BOS >, < EOS >, < UNK > \n",
    "    - < PAD >  ：無意義，將句子拓展到相同長度\n",
    "    - < BOS >  ：Begin of sentence, 開始字元\n",
    "    - < EOS >  ：End of sentence, 結尾字元\n",
    "    - < UNK >  ：單字沒有出現在字典裡的字\n",
    "  - 將字典裡每個 subword (詞) 用一個整數表示，分為英文和中文的字典，方便之後轉為 one-hot vector   \n",
    "  - 在將答案傳出去前，在答案開頭加入 \"< BOS >\" 符號，並於答案結尾加入 \"< EOS >\" 符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "class EN2CNDataset(data.Dataset): # 英文 -> 中文\n",
    "    def __init__(self, path, max_output_len, set_name):\n",
    "        self.path = path\n",
    "        self.word2int_cn, self.int2word_cn = self.get_dictionary('cn') # 載入转换字典\n",
    "        self.word2int_en, self.int2word_en = self.get_dictionary('en')\n",
    "        self.data = []\n",
    "        with open(os.path.join(self.path, f'{set_name}.txt'), \"r\", encoding='utf-8') as f: \n",
    "            for line in f:\n",
    "                self.data.append(line)\n",
    "        print (f'{set_name} dataset size: {len(self.data)}')\n",
    "        self.cn_vocab_size = len(self.word2int_cn) # 词库大小\n",
    "        self.en_vocab_size = len(self.word2int_en)\n",
    "        self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>']) # 填充\n",
    "\n",
    "    def get_dictionary(self, language): # 載入字典\n",
    "        with open(os.path.join(self.path, f'word2int_{language}.json'), \"r\",encoding='utf-8') as f: \n",
    "            word2int = json.load(f)\n",
    "        with open(os.path.join(self.path, f'int2word_{language}.json'), \"r\",encoding='utf-8') as f:\n",
    "            int2word = json.load(f)\n",
    "        return word2int, int2word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, Index): # 获得其中一句\n",
    "        sentences = self.data[Index]\n",
    "        sentences = re.split('[\\t\\n]', sentences) # 將中英文分開\n",
    "        sentences = list(filter(None, sentences)) # filter(函数, 数据源) 用函数过滤数据源，返回一个包含函数输出的迭代器，list()转换为列表\n",
    "        assert len(sentences) == 2 # assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常\n",
    "        BOS = self.word2int_en['<BOS>'] # 預備特殊字元 - 开始\n",
    "        EOS = self.word2int_en['<EOS>'] # 預備特殊字元 - 结束\n",
    "        UNK = self.word2int_en['<UNK>'] # 預備特殊字元 - 不在词向量里的词\n",
    "        en, cn = [BOS], [BOS] # 在開頭添加 <BOS>，在結尾添加 <EOS> ，不在字典的 subword (詞) 用 <UNK> 取代\n",
    "    \n",
    "        # 处理英文\n",
    "        sentence = re.split(' ', sentences[0]) # 將句子拆解為 subword 並轉為整數，空格分隔\n",
    "        sentence = list(filter(None, sentence))\n",
    "        for word in sentence: # 英文word 变成向量\n",
    "            en.append(self.word2int_en.get(word, UNK))\n",
    "        en.append(EOS) # 結尾添加 <EOS>\n",
    "    \n",
    "        # 处理中文\n",
    "        sentence = re.split(' ', sentences[1]) # 將句子拆解為單詞並轉為整數\n",
    "        sentence = list(filter(None, sentence))\n",
    "        for word in sentence:\n",
    "            cn.append(self.word2int_cn.get(word, UNK))\n",
    "        cn.append(EOS)\n",
    "\n",
    "        en, cn = np.asarray(en), np.asarray(cn)\n",
    "        # 用 <PAD> 將句子補到相同長度\n",
    "        en, cn = self.transform(en), self.transform(cn)\n",
    "        en, cn = torch.LongTensor(en), torch.LongTensor(cn)\n",
    "\n",
    "        return en, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架構\n",
    "![](hw8_data/总model图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- seq2seq模型的編碼器為RNN。 對於每個輸入，**Encoder** 會輸出**一個向量**和**一個隱藏狀態(hidden state)**，並將隱藏狀態用於下一個輸入，換句話說，**Encoder** 會逐步讀取輸入序列，並輸出單個矢量（最終隱藏狀態）\n",
    "- 參數:\n",
    "  - **en_vocab_size** 英文词库的大小，也就是英文的 subword 的個數\n",
    "  - **emb_dim** embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用，可以使用預先訓練好的 word embedding，如 Glove 和 word2vector\n",
    "  - **hid_dim** RNN 輸出和隱藏狀態的維度\n",
    "  - **n_layers** RNN 要疊多少層\n",
    "  - **dropout** 決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
    "- Encoder 的輸入和輸出:\n",
    "  - 輸入: \n",
    "    - 英文的整數序列 e.g. 1, 28, 29, 205, 2\n",
    "  - 輸出: \n",
    "    - outputs: 最上層 RNN 全部的輸出，可以用 Attention 再進行處理\n",
    "    - hidden: 每層最後的隱藏狀態，將傳遞到 Decoder 進行解碼\n",
    "![](hw8_data/Encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # 将用来表示词的整数转换成 词向量\n",
    "    def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(en_vocab_size, emb_dim)\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout = dropout, batch_first = True, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedding = self.embedding(input) # input[batch size, sequence len, vocab size]\n",
    "        outputs, hidden = self.rnn(self.dropout(embedding))\n",
    "        # outputs = [batch size, sequence len, hid dim * directions] 单向directions = 1，双向directions = 2，两个向量接起来\n",
    "        # hidden =  [num_layers * directions, batch size, hid dim]\n",
    "        # outputs 是最上層RNN的輸出，最终的输出\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- **Decoder** 是另一個 RNN，在最簡單的 seq2seq decoder 中，僅使用 **Encoder** 每一層最後的隱藏狀態來進行解碼，而這最後的隱藏狀態有時被稱為 “content vector”，因為可以想像它對整個前文序列進行編碼， 此 “content vector” 用作 **Decoder** 的**初始**隱藏狀態， 而 **Encoder** 的輸出通常用於 Attention Mechanism\n",
    "- 參數:\n",
    "  - **en_vocab_size** 英文词库的大小，也就是英文的 subword 的個數\n",
    "  - **emb_dim** embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用，可以使用預先訓練好的 word embedding，如 Glove 和 word2vector\n",
    "  - **hid_dim** RNN 輸出和隱藏狀態的維度\n",
    "  - **n_layers** RNN 要疊多少層\n",
    "  - **dropout** 決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
    "  - **isatt** 是來決定是否使用 Attention Mechanism\n",
    "\n",
    "- Decoder 的輸入和輸出:\n",
    "  - 輸入:\n",
    "    - 前一次解碼出來的單詞的整數表示\n",
    "  - 輸出:\n",
    "    - hidden: 根據輸入和前一次的隱藏狀態，現在的隱藏狀態更新的結果\n",
    "    - output: 每個字有多少機率是這次解碼的結果\n",
    "![](hw8_data/Decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cn_vocab_size, emb_dim, hid_dim, n_layers, dropout, isatt):\n",
    "        super().__init__()\n",
    "        self.cn_vocab_size = cn_vocab_size\n",
    "        self.hid_dim = hid_dim * 2 # 编码时正向、反向各一个向量，两个向量接起来，维度 * 2\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(cn_vocab_size, config.emb_dim)\n",
    "        self.isatt = isatt\n",
    "        self.attention = Attention(hid_dim)\n",
    "        # 如果使用 Attention Mechanism 會使得輸入維度變化，請在這裡修改\n",
    "        # e.g. Attention 接在輸入後面會使得維度變化，所以輸入維度改為\n",
    "        # self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\n",
    "        self.input_dim = emb_dim\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hid_dim, self.n_layers, dropout = dropout, batch_first = True)\n",
    "        self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim * 2)\n",
    "        self.embedding2vocab2 = nn.Linear(self.hid_dim * 2, self.hid_dim * 4)\n",
    "        self.embedding2vocab3 = nn.Linear(self.hid_dim * 4, self.cn_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, n layers * directions, hid dim]\n",
    "        # Decoder 只會是單向，所以 directions = 1\n",
    "        input = input.unsqueeze(1) # input[batch size, vocab size] 变成一行\n",
    "        embedded = self.dropout(self.embedding(input)) # embedded [batch size, 1, emb dim]\n",
    "        if self.isatt:\n",
    "            attn = self.attention(encoder_outputs, hidden) # 在這裡決定如何使用 Attention，e.g. 相加 或是 接在後面， 請注意維度變化\n",
    "        output, hidden = self.rnn(embedded, hidden) # output[batch size, 1, hid dim]， hidden[num_layers, batch size, hid dim]\n",
    "        # 將 RNN 的輸出轉為每個詞出現的機率\n",
    "        output = self.embedding2vocab1(output.squeeze(1))\n",
    "        output = self.embedding2vocab2(output)\n",
    "        prediction = self.embedding2vocab3(output) # prediction[batch size, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- 當輸入過長，或是單獨靠 “content vector” 無法取得整個輸入的意思時，用 Attention Mechanism 來提供 **Decoder** 更多的資訊\n",
    "- 主要是根據現在 **Decoder hidden state** ，去計算在 **Encoder outputs** 中，那些词与词之间联系的强弱關系，關系的數值是傳給 **Decoder** 額外的資訊 \n",
    "- 常見 Attention 的實作是用 Neural Network / Dot Product 來算 **Decoder hidden state** 和 **Encoder outputs** 之間的關係，再對所有算出來的數值做 **softmax** ，最後根據過完 **softmax** 的值對 **Encoder outputs** 做 **weight sum**\n",
    "- 一般來說是取 Encoder 最後一層的 hidden state 來做 attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): # 词与词之间联系的强弱關系\n",
    "    def __init__(self, hid_dim): \n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "        self.match_nn = nn.Sequential( # match 网络是有2层 hidden layer 的 神经网络\n",
    "            nn.Linear(self.hid_dim * 2, self.hid_dim * 4),\n",
    "            nn.Linear(self.hid_dim * 4, self.hid_dim * 2),\n",
    "            nn.Linear(self.hid_dim * 2, 1))\n",
    "    \n",
    "    def match(self, h, z):\n",
    "        alpha = self.match_nn(torch.cat((h, z), dim=1)) # 将矩阵 h, z 左右拼接\n",
    "        return alpha\n",
    "  \n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        # encoder_outputs[batch size, sequence len, hid dim * directions]\n",
    "        # decoder_hidden[num_layers, batch size, hid dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # 将 tensor 转置\n",
    "        sequence_len = encoder_outputs.shape[0] # 句子长度\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        alphas = torch.zeros(sequence_len, batch_size).to(self.device) # 相关性的矩阵，每个元素装一个词的相关性矩阵\n",
    "        for i in range(sequence_len):\n",
    "            h = encoder_outputs[i] # 一个词的词向量\n",
    "            z = decoder_hidden[-1] #  Encoder 最後一層的 hidden state，包含所有词的信息\n",
    "            alpha = self.match(h, z).squeeze() # 去掉维度为 1 的维度，使矩阵维度 -1 e.g. [[1,2]] -> [1,2]\n",
    "            alphas[i] = alpha\n",
    "        alphas = alphas.softmax(dim = 0)\n",
    "        attention = torch.zeros(batch_size, self.hid_dim).to(self.device)\n",
    "        for i in range(sequence_len):\n",
    "            alpha = alphas[i].unsqueeze(1) # 第二维增加一个维度 e.g. [[1,2],[3,4]] -> [[[1,2]],[[3,4]]\n",
    "            h = encoder_outputs[i]\n",
    "            attention = attention + h * alpha # 做weigh sum\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq to Seq\n",
    "- 由 **Encoder** 和 **Decoder** 組成\n",
    "- 接收輸入並傳給 **Encoder** \n",
    "- 將 **Encoder** 的輸出傳給 **Decoder**\n",
    "- 不斷地將 **Decoder** 的輸出傳回 **Decoder** ，進行解碼  \n",
    "- 當解碼完成後，將 **Decoder** 的輸出傳回 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.n_layers == decoder.n_layers,  \"Encoder and decoder must have equal number of layers!\"\n",
    "            \n",
    "    def forward(self, input, target, teacher_forcing_ratio): # teacher_forcing_ratio 是有多少機率使用正確答案來訓練\n",
    "        # input[batch size, input len, vocab size]， target[batch size, target len, vocab size]\n",
    "        batch_size = target.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device) # 準備一個儲存空間來儲存輸出\n",
    "        encoder_outputs, hidden = self.encoder(input) # 將輸入放入 Encoder\n",
    "        # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder， encoder_outputs 主要是使用在 Attention\n",
    "        # Encoder 是雙向的RNN hidden[num_layers * directions, batch size, hid dim]  --> [num_layers, directions, batch size, hid dim]\n",
    "        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1) # 转换矩阵形状\n",
    "        hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2) # 拼接矩阵\n",
    "        input = target[:, 0] # 取的 <BOS> token\n",
    "        preds = []\n",
    "        for t in range(1, target_len): # <BOS> 不用分析\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = random.random() <= teacher_forcing_ratio # 決定是否用正確答案來做訓練\n",
    "            top1 = output.argmax(1) # 取出機率最大的單詞\n",
    "            # 如果是 teacher force 則用正解訓練，反之用自己預測的單詞做預測\n",
    "            input = target[:, t] if teacher_force and t < target_len else top1\n",
    "            preds.append(top1.unsqueeze(1))\n",
    "        preds = torch.cat(preds, 1)\n",
    "        return outputs, preds\n",
    "\n",
    "    def inference(self, input, target): # Beam Search以heapq實現\n",
    "        # 在這裡實施 Beam Search\n",
    "        # 此函式的 batch size = 1  \n",
    "        # input  = [batch size, input len, vocab size]\n",
    "        # target = [batch size, target len, vocab size]\n",
    "        batch_size = input.shape[0]\n",
    "        input_len = input.shape[1] # 取得最大字數\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "        # 準備一個儲存空間來儲存輸出\n",
    "        outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(input) # 將輸入放入 Encoder\n",
    "        # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\n",
    "        # encoder_outputs 主要是使用在 Attention\n",
    "        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\n",
    "        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1) # 重构张量的维度\n",
    "        hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2) # 拼接\n",
    "        input = target[:, 0] # 取的 <BOS> token\n",
    "        if BEAM_SEARCH == False:\n",
    "            preds = [] # 用来存结果\n",
    "            for t in range(1, input_len):\n",
    "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "                outputs[:, t] = output # 將預測結果存起來\n",
    "                top1 = output.argmax(1) # 取出機率最大的單詞\n",
    "                input = top1\n",
    "                preds.append(top1.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "            return outputs, preds\n",
    "        else:\n",
    "            if BEAM_WIDTH <= 1:\n",
    "                print('BEAM_WIDTH <= 1 !!')\n",
    "            preds = []\n",
    "            for t in range(1, input_len):\n",
    "                if t == 1:\n",
    "                    output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "                    outputs[:,t] = output\n",
    "                    output = F.log_softmax(output, dim = 1)\n",
    "                    log_prob, indexs = torch.topk(output, BEAM_WIDTH)\n",
    "                    for k, (log_p, idx) in enumerate(zip(log_prob[0], indexs[0])):\n",
    "                        idx = idx.view(1)\n",
    "                        pq.heappush(preds, [-log_p.clone().item(), [idx], hidden.clone(), outputs.clone()])\n",
    "                else:\n",
    "                    temp = []\n",
    "                    for i in range(BEAM_WIDTH):\n",
    "                        cur_p, cur_tokens, cur_hidden, cur_outputs = preds[i]\n",
    "                        input = cur_tokens[-1]\n",
    "                        output, cur_hidden = self.decoder(input, cur_hidden, encoder_outputs)\n",
    "                        cur_outputs[:,t] = output\n",
    "                        output = F.log_softmax(output, dim = 1)\n",
    "                        log_prob, indexs = torch.topk(output, BEAM_WIDTH)\n",
    "                    for j, (log_p, idx) in enumerate(zip(log_prob[0], indexs[0])):\n",
    "                        temp_p = cur_p\n",
    "                        idx = idx.view(1)\n",
    "                        tmp_token = cur_tokens + [idx]\n",
    "                        temp_p -= log_p.item()\n",
    "                        pq.heappush(temp, [temp_p, tmp_token, cur_hidden.clone(), cur_outputs.clone()])\n",
    "                preds = [pq.heappop(temp) for i in range(BEAM_WIDTH)]\n",
    "                del temp\n",
    "            preds = pq.heappop(preds)\n",
    "            _, preds, _, outputs= preds\n",
    "            preds = torch.LongTensor(preds).view(1, len(preds))\n",
    "            return outputs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "- 基本操作:\n",
    "  - 儲存模型\n",
    "  - 載入模型\n",
    "  - 建構模型\n",
    "  - 將一連串的數字還原回句子\n",
    "  - 計算 BLEU score\n",
    "  - 迭代 dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 儲存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, store_model_path, step):\n",
    "    torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, load_model_path):\n",
    "    print(f'Load model from {load_model_path}')\n",
    "    model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, en_vocab_size, cn_vocab_size):\n",
    "    # 建構模型\n",
    "    encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout)\n",
    "    decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout, config.attention)\n",
    "    model = Seq2Seq(encoder, decoder, device)\n",
    "    print(model)\n",
    "    \n",
    "    # 建構 optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    print(optimizer)\n",
    "    if config.load_model:\n",
    "        model = load_model(model, config.load_model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 數字轉句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2sentence(outputs, int2word):\n",
    "    sentences = []\n",
    "    for tokens in outputs:\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            word = int2word[str(int(token))]\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "        sentence.append(word)\n",
    "    sentences.append(sentence)\n",
    "  \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算 BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def computebleu(sentences, targets):\n",
    "    score = 0 \n",
    "    assert (len(sentences) == len(targets))\n",
    "\n",
    "    def cut_token(sentence):\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\n",
    "                tmp.append(token)\n",
    "            else:\n",
    "                tmp += [word for word in token]\n",
    "        return tmp \n",
    "\n",
    "    for sentence, target in zip(sentences, targets):\n",
    "        sentence = cut_token(sentence)\n",
    "        target = cut_token(target)\n",
    "        score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_iter(data_loader):\n",
    "  it = iter(data_loader)\n",
    "  while True:\n",
    "    try:\n",
    "      ret = next(it)\n",
    "      yield ret\n",
    "    except StopIteration:\n",
    "      it = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schedule_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#選擇使用 Linear, Exponential, Inverse Sigmoid實現\n",
    "def schedule_sampling():\n",
    "    try:\n",
    "        mode, h_para = mode\n",
    "    except:\n",
    "        mode = MODE[0]\n",
    "    \n",
    "    if mode == 'Naive':\n",
    "        return TEACHER_FORCE_RATE\n",
    "    elif mode == 'Linear':\n",
    "        decrement = 1/t_step\n",
    "        return 1- decrement * step\n",
    "    elif mode == 'Exponential': # 0.999 \n",
    "        return h_para**step\n",
    "    elif mode == 'Inverse_Sigmoid':\n",
    "        return h_para / (h_para + np.exp(step/h_para)) # 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    losses = []\n",
    "    loss_sum = 0.0\n",
    "    for step in range(summary_steps):\n",
    "        sources, targets = next(train_iter)\n",
    "        sources, targets = sources.to(device), targets.to(device)\n",
    "        outputs, preds = model(sources, targets, schedule_sampling())\n",
    "        # targets 的第一個 token 是 <BOS> 所以忽略\n",
    "        outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n",
    "        targets = targets[:, 1:].reshape(-1)\n",
    "        loss = loss_function(outputs, targets)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        if (step + 1) % 5 == 0:\n",
    "            loss_sum = loss_sum / 5\n",
    "            print (\"\\r\", \"train [{}] loss: {:.3f}, Perplexity: {:.3f}\".format(total_steps + step + 1, loss_sum, np.exp(loss_sum)), end=\" \")\n",
    "            losses.append(loss_sum)\n",
    "            loss_sum = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢驗/測試\n",
    "防止訓練發生overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_function):\n",
    "    model.eval()\n",
    "    loss_sum, bleu_score= 0.0, 0.0\n",
    "    n = 0\n",
    "    result = []\n",
    "    for sources, targets in dataloader:\n",
    "        sources, targets = sources.to(device), targets.to(device)\n",
    "        batch_size = sources.size(0)\n",
    "        outputs, preds = model.inference(sources, targets)\n",
    "        # targets 的第一個 token 是 <BOS> 所以忽略\n",
    "        outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n",
    "        targets = targets[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # 將預測結果轉為文字\n",
    "        targets = targets.view(sources.size(0), -1)\n",
    "        preds = tokens2sentence(preds, dataloader.dataset.int2word_cn)\n",
    "        sources = tokens2sentence(sources, dataloader.dataset.int2word_en)\n",
    "        targets = tokens2sentence(targets, dataloader.dataset.int2word_cn)\n",
    "        for source, pred, target in zip(sources, preds, targets):\n",
    "            result.append((source, pred, target))\n",
    "            # 計算 Bleu Score\n",
    "        bleu_score += computebleu(preds, targets)\n",
    "\n",
    "    n += batch_size\n",
    "\n",
    "    return loss_sum / len(dataloader), bleu_score / n, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練流程\n",
    "先訓練，再檢驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(config):\n",
    "    # 準備訓練資料\n",
    "    train_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'training')\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size = config.batch_size, shuffle = True)\n",
    "    train_iter = infinite_iter(train_loader)\n",
    "    # 準備檢驗資料\n",
    "    val_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'validation')\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size = 1)\n",
    "    # 建構模型\n",
    "    model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    train_losses, val_losses, bleu_scores = [], [], []\n",
    "    total_steps = 0\n",
    "    while (total_steps < config.num_steps):\n",
    "        # 訓練模型\n",
    "        model, optimizer, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, train_dataset)\n",
    "        train_losses += loss\n",
    "        # 檢驗模型\n",
    "        val_loss, bleu_score, result = test(model, val_loader, loss_function)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        total_steps += config.summary_steps\n",
    "        print (\"\\r\", \"val [{}] loss: {:.3f}, Perplexity: {:.3f}, blue score: {:.3f}       \".format(total_steps, val_loss, np.exp(val_loss), bleu_score))\n",
    "    \n",
    "        # 儲存模型和結果\n",
    "        if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n",
    "            save_model(model, optimizer, config.store_model_path, total_steps)\n",
    "            with open(f'{config.store_model_path}/output_{total_steps}.txt', 'w', encoding='utf-8') as f:\n",
    "                for line in result:\n",
    "                      print (line, file=f)\n",
    "    \n",
    "    return train_losses, val_losses, bleu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process(config):\n",
    "    # 準備測試資料\n",
    "    test_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'testing')\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=1)\n",
    "    # 建構模型\n",
    "    model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\n",
    "    print (\"Finish build model\")\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval() # 測試模型\n",
    "    test_loss, bleu_score, result = test(model, test_loader, loss_function)\n",
    "    with open(f'hw8_data/test_output.txt', 'w',encoding='utf-8') as f: # 儲存結果\n",
    "        for line in result:\n",
    "            print (line, file=f)\n",
    "    return test_loss, bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurations(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 60\n",
    "        self.emb_dim = 256\n",
    "        self.hid_dim = 512\n",
    "        self.n_layers = 3\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 0.00005\n",
    "        self.max_output_len = 50              # 最後輸出句子的最大長度\n",
    "        self.num_steps = 12000                # 總訓練次數\n",
    "        self.store_steps = 300                # 訓練多少次後須儲存模型\n",
    "        self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n",
    "        self.load_model = True               # 是否需載入模型\n",
    "        self.store_model_path = \"hw8_data/model\"      # 儲存模型的位置\n",
    "        self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
    "        self.data_path = \"hw8_data/\"          # 資料存放的位置\n",
    "        self.attention = True                # 是否使用 Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "讀入參數\n",
    "進行訓練或是推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'batch_size': 60, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 5e-05, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 300, 'summary_steps': 300, 'load_model': False, 'store_model_path': 'hw8_data/model', 'load_model_path': None, 'data_path': 'hw8_data/', 'attention': False}\n",
      "testing dataset size: 2636\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(3922, 256)\n",
      "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3805, 256)\n",
      "    (attention): Attention(\n",
      "      (match_nn): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (rnn): GRU(256, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (embedding2vocab1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (embedding2vocab2): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (embedding2vocab3): Linear(in_features=4096, out_features=3805, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Finish build model\n",
      "test loss: 8.241486211467043, bleu_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "BEAM_SEARCH = False\n",
    "BEAM_WIDTH = 3\n",
    "TEACHER_FORCE_RATE = 0.5\n",
    "# MODE = ['Linear']\n",
    "# MODE = ['Exponential', 0.999]\n",
    "MODE = ['Inverse_Sigmoid', 800]\n",
    "\n",
    "# 在執行 Test 之前，請先行至 config 設定所要載入的模型位置\n",
    "if __name__ == '__main__':\n",
    "    config = configurations()\n",
    "    print ('config:\\n', vars(config))\n",
    "    test_loss, bleu_score = test_process(config)\n",
    "    print (f'test loss: {test_loss}, bleu_score: {bleu_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 圖形化訓練過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以圖表呈現 **訓練** 的 loss 變化趨勢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e3e7c173b830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'次數'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('次數')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以圖表呈現 **檢驗** 的 loss 變化趨勢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6044ce52eeb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'次數'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('次數')\n",
    "plt.ylabel('loss')\n",
    "plt.title('validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c8a955f70f03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'val_losses' is not defined"
     ]
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e1c91fdfee39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbleu_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'次數'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BLEU score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bleu_scores' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(bleu_scores)\n",
    "plt.xlabel('次數')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.title('BLEU score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
